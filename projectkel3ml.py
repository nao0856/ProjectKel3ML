# -*- coding: utf-8 -*-
"""ProjectKel3ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_TbO6fUdFyeuw1pdFYVQBRww2tJLV-52
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

"""## 1. Pemuatan Data (Data Loading)

### Subtask:
Memuat data dari file "Presentase Penyelesaian Tindak Pidana di Indonesia tahun 2021-2022.csv" ke dalam sebuah DataFrame pandas.

**Alasan**:
Menggunakan pustaka pandas untuk membaca file CSV adalah langkah pertama dan paling umum dalam analisis data. Ini memungkinkan kita untuk memanipulasi dan menganalisis data dengan mudah dalam struktur tabel.
"""

df_crime = pd.read_csv("Presentase Penyelesaian Tindak Pidana di Indonesia tahun 2021-2022.csv")

"""**Alasan**:
Menampilkan beberapa baris pertama dari DataFrame (`.head()`) untuk memverifikasi bahwa data telah dimuat dengan benar dan untuk mendapatkan gambaran awal tentang struktur data.
"""

display(df_crime.head())

"""## 2. Eksplorasi Data (Data Exploration)

### Subtask:
Menjelajahi data yang dimuat untuk memahami struktur, kolom, tipe data, dan mengidentifikasi potensi masalah seperti nilai yang hilang atau inkonsistensi. Ini akan membantu dalam langkah-langkah pembersihan dan pengolahan data berikutnya.

**Alasan**:
Menggunakan `.info()`, `.describe()`, `.isnull().sum()`, dan `.nunique()` memberikan ringkasan komprehensif tentang dataset. Kita dapat memeriksa tipe data, statistik deskriptif, keberadaan nilai null, dan jumlah nilai unik di setiap kolom untuk memastikan kualitas dan kelengkapan data.
"""

print("Informasi DataFrame:")
df_crime.info()
print("\nStatistik Deskriptif:")
display(df_crime.describe())
print("\nJumlah Nilai Null per Kolom:")
display(df_crime.isnull().sum())
print("\nJumlah Nilai Unik per Kolom:")
display(df_crime.nunique())

"""## 3. Pembersihan Data (Data Cleaning)

### Subtask:
Membersihkan data dengan menangani nilai yang hilang (jika ada), mengatasi potensi masalah tipe data, dan memastikan data dalam format yang dapat digunakan untuk analisis dan pemodelan.

**Alasan**:
Meskipun hasil eksplorasi menunjukkan tidak ada nilai null atau duplikat, penting untuk secara eksplisit memeriksa dan menangani duplikat. Ini adalah praktik terbaik untuk memastikan integritas data sebelum melanjutkan ke tahap analisis.
"""

# Berdasarkan hasil .info() dan .isnull().sum(), tidak ada data yang hilang.
# Selanjutnya, kita periksa baris duplikat.
duplicate_rows = df_crime.duplicated().sum()
print(f"Jumlah baris duplikat: {duplicate_rows}")

# Jika ada baris duplikat, hapus baris tersebut.
if duplicate_rows > 0:
    df_crime.drop_duplicates(inplace=True)
    print("Baris duplikat telah dihapus.")

# Verifikasi tipe data sudah sesuai untuk operasi numerik.
print("\nVerifikasi tipe data:")
df_crime.info()

"""## 4. Rekayasa Fitur (Feature Engineering)

### Subtask:
Mengubah data untuk membuat variabel target yang cocok untuk klasifikasi. Ini melibatkan perhitungan rata-rata persentase penyelesaian tindak pidana dan membuat variabel target kategoris yang mewakili **tingkat penanganan** tindak pidana.

**Alasan**:
Untuk melakukan klasifikasi, kita memerlukan variabel target yang kategoris. Dengan menghitung rata-rata persentase penyelesaian dari kedua tahun, kita mendapatkan satu metrik tunggal. Metrik ini kemudian dikonversi menjadi kategori ('Rendah', 'Sedang', 'Tinggi') berdasarkan ambang batas yang ditentukan, yang akan menjadi target prediksi model kita.
"""

# Hitung rata-rata persentase penyelesaian tindak pidana dari tahun 2021 dan 2022.
df_crime['Rata_Rata_Penyelesaian(%)'] = (df_crime['Penyelesaian tindak pidana 2021(%)'] + df_crime['Penyelesaian tindak pidana 2022(%)']) / 2

# Lihat distribusi data untuk menentukan ambang batas yang sesuai.
print("Deskripsi Rata-Rata Persentase Penyelesaian:")
print(df_crime['Rata_Rata_Penyelesaian(%)'].describe())

# Tentukan ambang batas (thresholds) berdasarkan kuartil atau nilai lain yang relevan.
# Contoh: Rendah < 55%, Sedang 55%-70%, Tinggi > 70%
batas_rendah = 55
batas_tinggi = 70

# Buat fungsi untuk mengklasifikasikan tingkat penanganan.
def klasifikasi_tingkat_penanganan(persentase):
    if persentase > batas_tinggi:
        return 'Tinggi'
    elif persentase >= batas_rendah and persentase <= batas_tinggi:
        return 'Sedang'
    else:
        return 'Rendah'

# Buat variabel target kategoris 'Tingkat_Penanganan'.
df_crime['Tingkat_Penanganan'] = df_crime['Rata_Rata_Penyelesaian(%)'].apply(klasifikasi_tingkat_penanganan)

# Pilih fitur (X) dan variabel target (y).
# Untuk kasus ini, kita akan mencoba memprediksi tingkat penanganan berdasarkan jumlah tindak pidana.
X = df_crime[['Rata_Rata_Penyelesaian(%)']]
y = df_crime['Tingkat_Penanganan']

# Tampilkan DataFrame dengan kolom baru, serta fitur dan target yang dipilih.
print("\nDataFrame dengan kolom baru:")
display(df_crime.head())
print("\nFitur (X) yang akan digunakan:")
display(X.head())
print("\nTarget (y) yang akan diprediksi:")
display(y.head())

"""## 5. Analisis Data Lanjutan

### Subtask:
Melakukan analisis data dasar pada data yang telah diolah untuk memahami distribusi tingkat penanganan, jumlah hitungan untuk setiap tingkat, dan mengidentifikasi pola atau korelasi potensial antara jumlah tindak pidana dan tingkat penanganannya.

**Alasan**:
Menghitung jumlah data untuk setiap kategori dalam kolom 'Tingkat_Penanganan' (`.value_counts()`) penting untuk mengetahui apakah dataset kita seimbang atau tidak. Ketidakseimbangan kelas dapat mempengaruhi kinerja model.
"""

print("Distribusi data untuk 'Tingkat_Penanganan':")
print(df_crime['Tingkat_Penanganan'].value_counts())

"""**Alasan**:
Mengelompokkan data berdasarkan 'Tingkat_Penanganan' dan menghitung statistik deskriptif untuk jumlah tindak pidana membantu kita memahami apakah ada hubungan antara volume kejahatan dengan efektivitas penanganannya di suatu wilayah.
"""

print("\nStatistik deskriptif jumlah tindak pidana berdasarkan 'Tingkat_Penanganan':")
display(df_crime.groupby('Tingkat_Penanganan')[['Jumlah Tindak Pidana 2021', 'Jumlah Tindak Pidana 2022']].describe())

"""## 6. Persiapan Data untuk Model

### Subtask:
Mempersiapkan data untuk pelatihan model. Ini akan melibatkan pemisahan fitur (X) dan variabel target (y), serta melakukan penskalaan fitur (feature scaling).

**Alasan**:
Penskalaan fitur (menggunakan `StandardScaler`) adalah langkah penting sebelum melatih banyak model machine learning. Ini mengubah fitur sehingga memiliki rata-rata 0 dan standar deviasi 1, yang membantu model (seperti Regresi Logistik atau SVM) untuk konvergen lebih cepat dan berkinerja lebih baik, karena tidak ada satu fitur pun yang mendominasi karena skalanya.
"""

# Inisialisasi StandardScaler
scaler = StandardScaler()

# Lakukan penskalaan pada fitur X
X_scaled = scaler.fit_transform(X)

# Tampilkan beberapa baris pertama dari fitur yang telah diskalakan
print("Fitur yang Telah Diskalakan (X_scaled):")
display(X_scaled[:5])

"""## 7. Pembagian Data

### Subtask:
Membagi data yang telah disiapkan (fitur X_scaled dan target y) menjadi set data latih (training) dan uji (testing) untuk mengevaluasi performa model klasifikasi.

**Alasan**:
Membagi data menjadi set training dan testing adalah praktik standar dalam machine learning untuk mendapatkan evaluasi yang objektif terhadap kinerja model. Model dilatih pada data training dan diuji pada data testing yang belum pernah dilihat sebelumnya. Penggunaan `stratify=y` memastikan bahwa proporsi kelas ('Rendah', 'Sedang', 'Tinggi') sama di kedua set data, yang sangat penting untuk dataset yang tidak seimbang.
"""

# Membagi data menjadi set training dan testing
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled,      # Fitur yang telah diskalakan
    y,             # Variabel target
    test_size=0.25,  # 25% dari data untuk testing
    random_state=42, # Untuk reproduktifitas hasil
    stratify=y       # Stratifikasi berdasarkan target untuk menjaga proporsi kelas
)

# Cetak ukuran dari setiap set data untuk verifikasi
print("Ukuran X_train:", X_train.shape)
print("Ukuran X_test:", X_test.shape)
print("Ukuran y_train:", y_train.shape)
print("Ukuran y_test:", y_test.shape)

"""## 8. Pelatihan Model

### Subtask:
Melatih model klasifikasi pada data latih untuk memprediksi tingkat penanganan tindak pidana. Kita akan menggunakan model Pohon Keputusan (Decision Tree).

**Alasan**:
Decision Tree adalah model yang baik untuk tugas klasifikasi karena mudah diinterpretasikan dan dapat menangani hubungan non-linear. Kita akan menginisialisasi model `DecisionTreeClassifier` dan melatihnya menggunakan data training (`X_train`, `y_train`).
"""

# Inisialisasi model Decision Tree Classifier
# random_state digunakan untuk memastikan hasil yang sama setiap kali dijalankan
model_dt = DecisionTreeClassifier(random_state=42)

# Latih model pada data training
model_dt.fit(X_train, y_train)

"""## 9. Evaluasi Model

### Subtask:
Mengevaluasi performa model Decision Tree yang telah dilatih pada data uji menggunakan metrik yang sesuai seperti akurasi, presisi, recall, dan F1-score.

**Alasan**:
Setelah model dilatih, kita perlu mengukur seberapa baik kinerjanya pada data baru. Akurasi, presisi, recall, dan F1-score adalah metrik standar untuk mengevaluasi model klasifikasi. Menggunakan `average='weighted'` penting untuk dataset yang tidak seimbang karena memberikan bobot pada setiap kelas berdasarkan jumlah sampelnya.
"""

# Gunakan model yang telah dilatih untuk memprediksi tingkat penanganan pada data uji
y_pred = model_dt.predict(X_test)

# Hitung dan cetak akurasi
accuracy = accuracy_score(y_test, y_pred)
print(f"Akurasi: {accuracy:.4f}")

# Hitung dan cetak presisi, recall, dan F1-score
# Gunakan 'weighted' average untuk klasifikasi multi-kelas dengan data tidak seimbang
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Presisi (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")
print(f"F1-score (weighted): {f1:.4f}")

"""## 10. Ringkasan dan Kesimpulan

### Temuan Utama dari Analisis Data

*   Dataset berisi statistik tindak pidana dan persentase penyelesaiannya untuk 35 wilayah kepolisian di Indonesia pada tahun 2021 dan 2022.
*   Data bersih tanpa nilai yang hilang atau baris duplikat.
*   Variabel target **'Tingkat_Penanganan'** berhasil dibuat dengan mengkategorikan rata-rata persentase penyelesaian menjadi 'Rendah' (<55%), 'Sedang' (55%-70%), dan 'Tinggi' (>70%).
*   Distribusi tingkat penanganan tidak seimbang: 22 wilayah diklasifikasikan sebagai 'Rendah', 10 'Sedang', dan hanya 3 'Tinggi'. Ketidakseimbangan ini perlu diperhatikan karena dapat mempengaruhi performa model.
*   Menariknya, berdasarkan nilai median, wilayah dengan tingkat penanganan 'Tinggi' cenderung memiliki jumlah kasus tindak pidana yang lebih tinggi dibandingkan dengan kategori 'Rendah' dan 'Sedang'.
*   Fitur jumlah tindak pidana telah distandarisasi menggunakan `StandardScaler` untuk persiapan model.
*   Sebuah model klasifikasi **Pohon Keputusan (Decision Tree)** telah dilatih untuk memprediksi 'Tingkat_Penanganan' berdasarkan jumlah tindak pidana.
*   Model yang dilatih mencapai **akurasi 0.6667** pada data uji. Metrik presisi, recall, dan F1-score (weighted) juga menunjukkan nilai yang sama.

### Wawasan dan Langkah Selanjutnya

*   Performa model saat ini cukup moderat. Untuk meningkatkan performa, dapat dicoba teknik untuk menangani ketidakseimbangan kelas, seperti oversampling (misalnya, SMOTE) pada kelas minoritas ('Tinggi' dan 'Sedang').
*   Eksplorasi model klasifikasi lain (misalnya, Random Forest, Gradient Boosting, atau SVM) dapat dilakukan untuk membandingkan dan mencari model dengan performa terbaik.
*   Analisis lebih mendalam bisa dilakukan untuk menyelidiki mengapa daerah dengan tingkat penyelesaian yang tinggi juga memiliki jumlah tindak pidana yang tinggi. Mungkin ada faktor lain yang tidak termasuk dalam dataset ini, seperti jumlah personel polisi, alokasi anggaran, atau faktor sosio-ekonomi wilayah.
"""